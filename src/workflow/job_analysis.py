"""Job Detail Analysis Workflow."""

import re
import time
from typing import Dict, Any, Optional
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, TimeoutError as PlaywrightTimeout
from ..llm.client import call_llm
from .prompt import JOB_DETAIL_ANALYSIS_PROMPT


def extract_url_from_query(query: str) -> Optional[str]:
    """
    å¾æŸ¥è©¢å­—ä¸²ä¸­æå– URLã€‚

    Args:
        query: ç”¨æˆ¶æŸ¥è©¢å­—ä¸²

    Returns:
        æå–åˆ°çš„ URLï¼Œå¦‚æœæ²’æœ‰å‰‡è¿”å› None
    """
    # URL æ­£å‰‡è¡¨é”å¼
    url_pattern = r'https?://[^\s]+'
    match = re.search(url_pattern, query)
    return match.group(0) if match else None


def validate_url_security(url: str) -> bool:
    """
    é©—è­‰ URL çš„å®‰å…¨æ€§ï¼Œé˜²æ­¢ SSRF ç­‰æ”»æ“Šã€‚

    Args:
        url: è¦é©—è­‰çš„ URL

    Returns:
        URL æ˜¯å¦å®‰å…¨
    """
    try:
        parsed = urlparse(url)

        # åªå…è¨± http å’Œ https å”è­°
        if parsed.scheme not in ['http', 'https']:
            print(f"âš ï¸  ä¸æ”¯æ´çš„å”è­°ï¼š{parsed.scheme}")
            return False

        # ç¦æ­¢è¨ªå•å…§ç¶² IP å’Œ localhost
        hostname = parsed.hostname
        if not hostname:
            return False

        # ç¦æ­¢è¨ªå• localhost å’Œå…§ç¶² IP
        blocked_hosts = ['localhost', '127.0.0.1', '0.0.0.0']
        if hostname.lower() in blocked_hosts:
            print("âš ï¸  ç¦æ­¢è¨ªå• localhost")
            return False

        # ç¦æ­¢è¨ªå•å…§ç¶² IP æ®µï¼ˆ10.x.x.x, 172.16-31.x.x, 192.168.x.xï¼‰
        if hostname.startswith(('10.', '172.', '192.168.')):
            print("âš ï¸  ç¦æ­¢è¨ªå•å…§ç¶² IP")
            return False

        return True

    except Exception as e:
        print(f"âš ï¸  URL é©—è­‰å¤±æ•—ï¼š{e}")
        return False


def extract_104_job_content(soup: BeautifulSoup) -> str:
    """
    å°ˆé–€æå– 104 è·ç¼ºé é¢çš„å…§å®¹ã€‚

    Args:
        soup: BeautifulSoup ç‰©ä»¶

    Returns:
        æå–çš„è·ç¼ºè³‡è¨Š
    """
    info_parts = []

    try:
        # æå–è·ä½æ¨™é¡Œ
        title = soup.find('h1')
        if title:
            info_parts.append(f"è·ä½ï¼š{title.get_text(strip=True)}")

        # æå–å…¬å¸åç¨±
        company = soup.find('a', class_='company')
        if not company:
            company = soup.find(attrs={'data-qa': 'company-name'})
        if company:
            info_parts.append(f"å…¬å¸ï¼š{company.get_text(strip=True)}")

        # æå–è–ªè³‡
        salary = soup.find(attrs={'data-qa': 'salary'})
        if not salary:
            salary = soup.find('span', class_='salary')
        if salary:
            info_parts.append(f"è–ªè³‡ï¼š{salary.get_text(strip=True)}")

        # æå–å·¥ä½œåœ°é»
        location = soup.find(attrs={'data-qa': 'job-location'})
        if not location:
            location = soup.find('span', class_='location')
        if location:
            info_parts.append(f"åœ°é»ï¼š{location.get_text(strip=True)}")

        # æå–è·ä½æè¿°
        description = soup.find('div', class_='job-description')
        if not description:
            description = soup.find(attrs={'data-qa': 'job-description'})
        if description:
            desc_text = description.get_text(strip=True)[:500]  # é™åˆ¶é•·åº¦
            info_parts.append(f"è·ä½æè¿°ï¼š{desc_text}")

        return "\n".join(info_parts) if info_parts else ""

    except Exception as e:
        print(f"âš ï¸  104 å…§å®¹æå–å¤±æ•—ï¼š{e}")
        return ""


def extract_general_content(soup: BeautifulSoup) -> str:
    """
    æå–ä¸€èˆ¬ç¶²ç«™çš„ä¸»è¦å…§å®¹ã€‚

    Args:
        soup: BeautifulSoup ç‰©ä»¶

    Returns:
        æå–çš„æ–‡å­—å…§å®¹
    """
    try:
        # ç§»é™¤ script å’Œ style æ¨™ç±¤
        for script in soup(['script', 'style', 'header', 'footer', 'nav']):
            script.decompose()

        # æå–æ¨™é¡Œ
        title = soup.find('title')
        title_text = title.get_text(strip=True) if title else ""

        # æå– h1
        h1 = soup.find('h1')
        h1_text = h1.get_text(strip=True) if h1 else ""

        # æå–ä¸»è¦å…§å®¹å€åŸŸ
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)

        # æ¸…ç†å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # çµ„åˆè³‡è¨Š
        info_parts = []
        if title_text:
            info_parts.append(f"é é¢æ¨™é¡Œï¼š{title_text}")
        if h1_text and h1_text != title_text:
            info_parts.append(f"ä¸»æ¨™é¡Œï¼š{h1_text}")
        if text:
            # é™åˆ¶å…§å®¹é•·åº¦
            content_preview = text[:1500]
            info_parts.append(f"å…§å®¹ï¼š{content_preview}")

        return "\n".join(info_parts)

    except Exception as e:
        print(f"âš ï¸  é€šç”¨å…§å®¹æå–å¤±æ•—ï¼š{e}")
        return ""


def is_dynamic_website(url: str) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦ç‚ºéœ€è¦ JavaScript æ¸²æŸ“çš„å‹•æ…‹ç¶²ç«™ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™
    """
    # éœ€è¦ä½¿ç”¨ Playwright çš„ç¶²ç«™åˆ—è¡¨
    dynamic_domains = [
        '104.com.tw',
        # æœªä¾†å¯ä»¥æ·»åŠ æ›´å¤šéœ€è¦å‹•æ…‹æ¸²æŸ“çš„ç¶²ç«™
    ]
    return any(domain in url for domain in dynamic_domains)


async def extract_104_dynamic_content(page: Page) -> str:
    """
    ä½¿ç”¨ Playwright æå– 104 è·ç¼ºé é¢çš„çµæ§‹åŒ–å…§å®¹ã€‚

    æ­¤å‡½æ•¸æœƒæå–å®Œæ•´çš„è·ä½è³‡è¨Šï¼ŒåŒ…æ‹¬ï¼š
    - åŸºæœ¬è³‡è¨Šï¼šè·ä½ã€å…¬å¸ã€æ›´æ–°æ—¥æœŸ
    - å·¥ä½œå…§å®¹ï¼šå®Œæ•´çš„å·¥ä½œæè¿°å’Œè¦æ±‚
    - è·å‹™é¡åˆ¥
    - å·¥ä½œæ¢ä»¶ï¼šå¾…é‡ã€æ€§è³ªã€åœ°é»ã€æ™‚æ®µç­‰
    - æ¢ä»¶è¦æ±‚ï¼šå­¸æ­·ã€ç¶“æ­·ã€æŠ€èƒ½ã€å·¥å…·ç­‰
    - ç¦åˆ©åˆ¶åº¦
    - è¯çµ¡æ–¹å¼

    Args:
        page: Playwright Page ç‰©ä»¶

    Returns:
        æ ¼å¼åŒ–çš„è·ç¼ºè³‡è¨Šå­—ä¸²
    """
    try:
        # ç­‰å¾…é é¢å®Œå…¨è¼‰å…¥
        await page.wait_for_selector('body', timeout=15000)
        await page.wait_for_timeout(3000)

        # ç²å–æ•´å€‹é é¢çš„æ–‡æœ¬å…§å®¹
        body_text = await page.locator('body').inner_text()
        lines = [line.strip() for line in body_text.split('\n') if line.strip()]

        # å˜—è©¦å¾è¡Œä¸­æ‰¾åˆ°è·ä½å’Œå…¬å¸
        title = None
        company = None

        # æå– h1 æ¨™é¡Œä½œç‚ºè·ä½
        try:
            h1 = await page.locator('h1').first.inner_text()
            if h1 and len(h1) < 100:  # ç¢ºä¿æ˜¯åˆç†çš„è·ä½åç¨±
                title = h1.strip()
        except Exception:
            pass

        # å¦‚æœæ²’æ‰¾åˆ° h1ï¼Œå¾æ–‡æœ¬ä¸­æ¨æ–·
        if not title:
            for i, line in enumerate(lines):
                if 'å…¬å¸' in line and i > 0:
                    # ä¸Šä¸€è¡Œå¯èƒ½æ˜¯è·ä½
                    potential_title = lines[i-1]
                    if len(potential_title) < 50 and not any(x in potential_title for x in ['å·¥ä½œ', 'ç™»å…¥', 'APP', 'è·æ¶¯']):
                        title = potential_title
                        break

        # æŸ¥æ‰¾å…¬å¸åç¨±ï¼ˆé€šå¸¸åœ¨é–‹é ­éƒ¨åˆ†ï¼‰
        for i, line in enumerate(lines[:30]):
            if 'è‚¡ä»½æœ‰é™å…¬å¸' in line or 'æœ‰é™å…¬å¸' in line:
                if len(line) < 50:  # ç¢ºä¿æ˜¯å…¬å¸åç¨±è€Œä¸æ˜¯åœ°å€
                    company = line
                    break

        # æ§‹å»ºçµæ§‹åŒ–è³‡è¨Š
        structured_info = []

        if title:
            structured_info.append(f"ã€è·ä½åç¨±ã€‘\n{title}\n")

        if company:
            structured_info.append(f"ã€å…¬å¸åç¨±ã€‘\n{company}\n")

        # æå–å„å€‹é—œéµsection
        sections_to_extract = {
            'å·¥ä½œå…§å®¹': ('å·¥ä½œå…§å®¹', 'è·å‹™é¡åˆ¥'),
            'è·å‹™é¡åˆ¥': ('è·å‹™é¡åˆ¥', 'å·¥ä½œå¾…é‡'),
            'å·¥ä½œå¾…é‡': ('å·¥ä½œå¾…é‡', 'å·¥ä½œæ€§è³ª'),
            'å·¥ä½œæ¢ä»¶': ('å·¥ä½œæ€§è³ª', 'æ¢ä»¶è¦æ±‚'),
            'æ¢ä»¶è¦æ±‚': ('æ¢ä»¶è¦æ±‚', 'å…¬å¸ç’°å¢ƒç…§ç‰‡'),
            'ç¦åˆ©åˆ¶åº¦': ('ç¦åˆ©åˆ¶åº¦', 'è¯çµ¡æ–¹å¼'),
            'è¯çµ¡æ–¹å¼': ('è¯çµ¡æ–¹å¼', '104äººåŠ›éŠ€è¡Œæé†’æ‚¨'),
        }

        for section_name, (start_marker, end_marker) in sections_to_extract.items():
            section_content = extract_section(lines, start_marker, end_marker)
            if section_content:
                structured_info.append(f"ã€{section_name}ã€‘\n{section_content}\n")

        # å¦‚æœæˆåŠŸæå–äº†çµæ§‹åŒ–è³‡è¨Šï¼Œè¿”å›
        if len(structured_info) > 2:  # è‡³å°‘æœ‰æ¨™é¡Œã€å…¬å¸å’Œä¸€å€‹section
            result = "\n".join(structured_info)
            print(f"âœ… æˆåŠŸæå–çµæ§‹åŒ–å…§å®¹ï¼ˆ{len(result)} å­—å…ƒï¼‰")
            return result

        # å¦‚æœçµæ§‹åŒ–æå–å¤±æ•—ï¼Œè¿”å›è™•ç†éçš„å®Œæ•´æ–‡æœ¬
        print("âš ï¸  çµæ§‹åŒ–æå–ä¸å®Œæ•´ï¼Œä½¿ç”¨å®Œæ•´æ–‡æœ¬")
        # æ¸…ç†æ–‡æœ¬ï¼šç§»é™¤å°èˆªã€é å°¾ç­‰ç„¡é—œå…§å®¹
        cleaned_lines = []
        skip_patterns = ['ç™»å…¥', 'è¨»å†Š', '104äººåŠ›éŠ€è¡Œ', 'APP', 'æ‰€æœ‰æœå‹™', 'æ™ºèƒ½å®¢æœ',
                        'å¸¸è¦‹å•é¡Œ', 'éš±ç§ä¸­å¿ƒ', 'å»ºè­°ç€è¦½å™¨', 'é©åˆä½ å¤§å±•èº«æ‰‹',
                        'æˆ‘é©åˆé€™ä»½å·¥ä½œå—', 'é€™äº›å·¥ä½œä¹Ÿå¾ˆé©åˆä½ ']

        in_content = False
        for line in lines:
            # é–‹å§‹å…§å®¹æ¨™è¨˜
            if any(marker in line for marker in ['å·¥ä½œå…§å®¹', 'è·å‹™é¡åˆ¥', 'å…¬å¸åç¨±']) and not in_content:
                in_content = True

            # è·³éç„¡é—œå…§å®¹
            if any(pattern in line for pattern in skip_patterns):
                continue

            # çµæŸæ¨™è¨˜
            if '104äººåŠ›éŠ€è¡Œæé†’æ‚¨' in line or 'è·å ´å®‰å…¨æé†’' in line:
                break

            if in_content and line:
                cleaned_lines.append(line)

        cleaned_text = '\n'.join(cleaned_lines)
        print(f"âœ… æå–æ¸…ç†å¾Œçš„æ–‡æœ¬ï¼ˆ{len(cleaned_text)} å­—å…ƒï¼‰")
        return cleaned_text

    except Exception as e:
        print(f"âš ï¸  104 å‹•æ…‹å…§å®¹æå–å¤±æ•—ï¼š{e}")
        import traceback
        traceback.print_exc()
        return ""


def extract_section(lines: list, start_marker: str, end_marker: str) -> str:
    """
    å¾æ–‡æœ¬è¡Œåˆ—è¡¨ä¸­æå–ç‰¹å®šsectionçš„å…§å®¹ã€‚

    Args:
        lines: æ–‡æœ¬è¡Œåˆ—è¡¨
        start_marker: sectioné–‹å§‹æ¨™è¨˜
        end_marker: sectionçµæŸæ¨™è¨˜

    Returns:
        æå–çš„sectionå…§å®¹
    """
    content_lines = []
    capturing = False

    # éæ¿¾æ¨¡å¼ï¼šç§»é™¤å†—é¤˜çš„éˆæ¥æ–‡å­—ï¼ˆåƒ…ç”¨æ–¼è·å‹™é¡åˆ¥sectionï¼‰
    link_filter_patterns = [
        'èªè­˜', 'æ›´å¤š', 'ç›¸é—œå·¥ä½œ', '-æ±‚è·é¢è©¦å·¥ä½œå•é¡Œ',
        'è·å‹™åˆ†æ', 'æŠ€èƒ½è­‰ç…§', 'è–ªè³‡å’Œå­¸ç¿’'
    ]

    for line in lines:
        if start_marker in line:
            capturing = True
            # å¦‚æœé€™è¡Œåªæœ‰æ¨™è¨˜ï¼Œè·³éï¼›å¦å‰‡åŒ…å«é€™è¡Œ
            if line.strip() != start_marker:
                content_lines.append(line)
            continue

        if capturing:
            if end_marker in line:
                break

            # å°æ–¼è·å‹™é¡åˆ¥sectionï¼Œéæ¿¾æ‰éˆæ¥æ–‡å­—
            if start_marker == 'è·å‹™é¡åˆ¥':
                if any(pattern in line for pattern in link_filter_patterns):
                    continue
                # éæ¿¾æ‰åªæœ‰é “è™Ÿçš„è¡Œ
                if line.strip() == 'ã€':
                    continue

            content_lines.append(line)

    return '\n'.join(content_lines).strip()


async def fetch_dynamic_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ Playwright æå–å‹•æ…‹ç¶²ç«™å…§å®¹ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æå–çš„å…§å®¹ï¼Œå¤±æ•—å‰‡è¿”å› None
    """
    try:
        print(f"ğŸ­ ä½¿ç”¨ Playwright æå–å‹•æ…‹å…§å®¹ï¼š{url}")

        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨ï¼ˆheadless æ¨¡å¼ï¼‰
            browser = await p.chromium.launch(headless=True)

            # å‰µå»ºä¸Šä¸‹æ–‡ï¼Œç¦ç”¨ä¸å¿…è¦çš„è³‡æºä»¥æé«˜é€Ÿåº¦
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                ignore_https_errors=False,  # é©—è­‰ SSL
            )

            # é˜»æ­¢ä¸å¿…è¦çš„è³‡æºè¼‰å…¥ä»¥æé«˜é€Ÿåº¦
            await context.route("**/*.{png,jpg,jpeg,gif,svg,css,font,woff,woff2}", lambda route: route.abort())

            # å‰µå»ºé é¢
            page = await context.new_page()

            # è¨­ç½®è¶…æ™‚
            page.set_default_timeout(30000)  # 30 ç§’

            # è¨ªå•é é¢ - ä½¿ç”¨ domcontentloaded è€Œä¸æ˜¯ networkidleï¼ˆæ›´å¿«ï¼‰
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)

            # åˆ¤æ–·æ˜¯å¦ç‚º 104 ç¶²ç«™
            is_104 = '104.com.tw' in url

            if is_104:
                print("ğŸ“‹ ä½¿ç”¨ 104 å°ˆç”¨æå–å™¨")
                content = await extract_104_dynamic_content(page)
            else:
                print("ğŸ“„ ä½¿ç”¨é€šç”¨å‹•æ…‹å…§å®¹æå–å™¨")
                # é€šç”¨æå–ï¼šç­‰å¾…é é¢ç©©å®šå¾Œç²å–æ–‡å­—
                await page.wait_for_timeout(2000)
                body_text = await page.locator('body').inner_text()
                body_text = re.sub(r'\s+', ' ', body_text).strip()[:1500]
                content = f"é é¢å…§å®¹ï¼š{body_text}"

            # é—œé–‰ç€è¦½å™¨
            await browser.close()

            if not content:
                print("âš ï¸  æœªèƒ½æå–åˆ°æœ‰æ•ˆå…§å®¹")
                return None

            print(f"âœ… æˆåŠŸæå–å‹•æ…‹å…§å®¹ï¼ˆ{len(content)} å­—å…ƒï¼‰")
            return content

    except PlaywrightTimeout:
        print("âš ï¸  Playwright è«‹æ±‚è¶…æ™‚")
        return None
    except Exception as e:
        print(f"âš ï¸  Playwright æå–å¤±æ•—ï¼š{e}")
        return None


def fetch_static_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ requests + BeautifulSoup æå–éœæ…‹ç¶²ç«™å…§å®¹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æå–çš„å…§å®¹ï¼Œå¤±æ•—å‰‡è¿”å› None
    """
    try:
        print(f"ğŸŒ ä½¿ç”¨éœæ…‹æ–¹æ³•æå–ç¶²é å…§å®¹ï¼š{url}")

        # è¨­å®šè«‹æ±‚æ¨™é ­
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        }

        # ç™¼é€è«‹æ±‚
        response = requests.get(
            url,
            headers=headers,
            timeout=10,
            allow_redirects=True,
            verify=True,
            stream=True
        )
        response.raise_for_status()

        # é™åˆ¶å…§å®¹å¤§å°
        content_length = response.headers.get('content-length')
        if content_length and int(content_length) > 5 * 1024 * 1024:
            print("âš ï¸  ç¶²é å…§å®¹éå¤§ï¼Œè¶…é 5MB é™åˆ¶")
            return None

        # ç²å–å…§å®¹
        content = b''
        size = 0
        max_size = 5 * 1024 * 1024
        for chunk in response.iter_content(chunk_size=8192):
            size += len(chunk)
            if size > max_size:
                print("âš ï¸  ä¸‹è¼‰å…§å®¹è¶…éå¤§å°é™åˆ¶")
                break
            content += chunk

        # è§£æ HTML
        soup = BeautifulSoup(content, 'html.parser')
        extracted_content = extract_general_content(soup)

        if not extracted_content:
            print("âš ï¸  æœªèƒ½æå–åˆ°æœ‰æ•ˆå…§å®¹")
            return None

        # é•·åº¦é™åˆ¶
        if len(extracted_content) > 2000:
            extracted_content = extracted_content[:2000] + "..."

        print(f"âœ… æˆåŠŸæå–éœæ…‹å…§å®¹ï¼ˆ{len(extracted_content)} å­—å…ƒï¼‰")
        return extracted_content

    except requests.Timeout:
        print("âš ï¸  è«‹æ±‚è¶…æ™‚")
        return None
    except requests.RequestException as e:
        print(f"âš ï¸  ç¶²é æå–å¤±æ•—ï¼š{e}")
        return None
    except Exception as e:
        print(f"âš ï¸  è™•ç†ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None


async def fetch_webpage_content(url: str) -> Optional[str]:
    """
    æ™ºèƒ½æå–ç¶²é å…§å®¹ï¼Œè‡ªå‹•é¸æ“‡éœæ…‹æˆ–å‹•æ…‹æå–æ–¹æ³•ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰ã€‚

    æµç¨‹ï¼š
    1. URL å®‰å…¨é©—è­‰
    2. åˆ¤æ–·æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™ï¼ˆå¦‚ 104.com.twï¼‰
    3. å‹•æ…‹ç¶²ç«™ â†’ ä½¿ç”¨ Playwrightï¼ˆç•°æ­¥ï¼‰
    4. éœæ…‹ç¶²ç«™ â†’ ä½¿ç”¨ requests + BeautifulSoup

    è³‡å®‰é˜²è­·æªæ–½ï¼š
    - URL å®‰å…¨é©—è­‰ï¼ˆé˜²æ­¢ SSRF æ”»æ“Šï¼‰
    - é™åˆ¶è«‹æ±‚å¤§å°
    - è¨­å®šè¶…æ™‚æ™‚é–“
    - é©—è­‰ SSL è­‰æ›¸

    Args:
        url: ç¶²é  URL

    Returns:
        ç¶²é çš„æ–‡å­—å…§å®¹æ‘˜è¦ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    # è³‡å®‰æª¢æŸ¥ï¼šé©—è­‰ URL å®‰å…¨æ€§
    if not validate_url_security(url):
        print("âŒ URL å®‰å…¨é©—è­‰å¤±æ•—")
        return None

    # åˆ¤æ–·æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™
    if is_dynamic_website(url):
        # ä½¿ç”¨ Playwright æå–å‹•æ…‹å…§å®¹ï¼ˆç•°æ­¥ï¼‰
        return await fetch_dynamic_content(url)
    else:
        # ä½¿ç”¨éœæ…‹æ–¹æ³•æå–å…§å®¹
        return fetch_static_content(url)


async def analyze_job_detail(job_query: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM åˆ†æç‰¹å®šè·ç¼ºçš„è©³ç´°è³‡è¨Šã€‚

    æ­¤å‡½æ•¸åˆ©ç”¨ LLM å…§å»ºçš„ web search åŠŸèƒ½ï¼ŒæŸ¥è©¢è·ç¼ºç›¸é—œçš„å…¬å¸èƒŒæ™¯ã€
    è·ä½è¦æ±‚ã€å“¡å·¥è©•åƒ¹ç­‰è³‡è¨Šï¼Œä¸¦ç”Ÿæˆåˆ†æå ±å‘Šã€‚

    Args:
        job_query: è·ç¼ºæŸ¥è©¢è³‡è¨Šï¼Œæ‡‰åŒ…å«è·ä½åç¨±ã€å…¬å¸åç¨±ç­‰åŸºæœ¬æè¿°ã€‚
                  ç¯„ä¾‹ï¼šã€ŒæŸç§‘æŠ€å…¬å¸çš„ Python å¾Œç«¯å·¥ç¨‹å¸«ã€æˆ–
                        ã€Œ104 è·ç¼ºé€£çµï¼šhttps://www.104.com.tw/job/xxxxxã€

    Returns:
        åŒ…å«åˆ†æçµæœçš„å­—å…¸ï¼š
        {
            "job_query": str,                # è¼¸å…¥çš„è·ç¼ºæŸ¥è©¢
            "analysis_report": str,          # LLM ç”Ÿæˆçš„åˆ†æå ±å‘Š
            "processing_time": float,        # è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰
            "token_usage": Dict,             # Token ä½¿ç”¨é‡çµ±è¨ˆ
        }

    Example:
        >>> result = analyze_job_detail("æŸç§‘æŠ€å…¬å¸çš„ Python å¾Œç«¯å·¥ç¨‹å¸«")
        >>> print(result["analysis_report"])

        >>> # ä¹Ÿå¯ä»¥ç›´æ¥å‚³å…¥ 104 é€£çµ
        >>> result = analyze_job_detail("https://www.104.com.tw/job/xxxxx")
        >>> print(result["analysis_report"])
    """

    # æº–å‚™è¿”å›çµæœ
    result = {
        "job_query": job_query,
        "analysis_report": "",
        "processing_time": 0.0,
        "token_usage": {},
        "webpage_content": None,  # æ–°å¢ï¼šå„²å­˜æå–çš„ç¶²é å…§å®¹
    }

    # æª¢æŸ¥æ˜¯å¦åŒ…å« URL
    url = extract_url_from_query(job_query)
    webpage_content = None

    if url:
        # å¦‚æœåŒ…å« URLï¼Œå…ˆæå–ç¶²é å…§å®¹
        webpage_content = await fetch_webpage_content(url)
        print(webpage_content)
        result["webpage_content"] = webpage_content

    # æº–å‚™è·ç¼ºè³‡è¨Š
    if webpage_content:
        # å¦‚æœæˆåŠŸæå–ç¶²é å…§å®¹ï¼Œå°‡å…¶åŠ å…¥æŸ¥è©¢è³‡è¨Š
        formatted_info = f"""ç”¨æˆ¶æŸ¥è©¢ï¼š{job_query}

å¾è·ç¼ºç¶²é æå–çš„åŸºæœ¬è³‡è¨Šï¼š
{webpage_content}

è«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šé€²è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚"""
    else:
        # æ²’æœ‰ç¶²é å…§å®¹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è©¢
        formatted_info = job_query

    # ä½¿ç”¨ LLM é€²è¡Œåˆ†æ
    print("ğŸ¤– åˆ†æè·ç¼ºè©³ç´°è³‡è¨Šä¸­...")

    messages = [
        {
            "role": "system",
            "content": JOB_DETAIL_ANALYSIS_PROMPT.format(job_info=formatted_info),
        },
        {
            "role": "user",
            "content": "è«‹é–‹å§‹åˆ†æé€™å€‹è·ç¼ºã€‚",
        },
    ]

    start_time = time.time()
    llm_response = call_llm(messages=messages)
    processing_time = time.time() - start_time

    result["processing_time"] = processing_time

    # é¡¯ç¤ºè™•ç†æ™‚é–“å’Œ token ä½¿ç”¨é‡
    print(f"â±ï¸  åˆ†æè€—æ™‚: {processing_time:.2f} ç§’")
    if "usage" in llm_response:
        usage = llm_response["usage"]
        result["token_usage"] = {
            "total": usage.get("total_tokens", 0),
            "prompt": usage.get("prompt_tokens", 0),
            "completion": usage.get("completion_tokens", 0),
        }
        print(f"ğŸ“Š Token ä½¿ç”¨é‡: {usage.get('total_tokens', 0)} tokens "
              f"(prompt: {usage.get('prompt_tokens', 0)}, "
              f"completion: {usage.get('completion_tokens', 0)})")

    # æå–åˆ†æå ±å‘Š
    result["analysis_report"] = llm_response["choices"][0]["message"]["content"]
    print(f"ğŸ“ å ±å‘Šé•·åº¦ï¼š{len(result['analysis_report'])} å­—å…ƒ")

    return result
