"""Job Detail Analysis Workflow."""

import re
import time
from typing import Dict, Any, Optional
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Page, TimeoutError as PlaywrightTimeout
from ..llm.client import call_llm
from .prompt import JOB_DETAIL_ANALYSIS_PROMPT


def extract_url_from_query(query: str) -> Optional[str]:
    """
    å¾æŸ¥è©¢å­—ä¸²ä¸­æå– URLã€‚

    Args:
        query: ç”¨æˆ¶æŸ¥è©¢å­—ä¸²

    Returns:
        æå–åˆ°çš„ URLï¼Œå¦‚æœæ²’æœ‰å‰‡è¿”å› None
    """
    # URL æ­£å‰‡è¡¨é”å¼
    url_pattern = r'https?://[^\s]+'
    match = re.search(url_pattern, query)
    return match.group(0) if match else None


def validate_url_security(url: str) -> bool:
    """
    é©—è­‰ URL çš„å®‰å…¨æ€§ï¼Œé˜²æ­¢ SSRF ç­‰æ”»æ“Šã€‚

    Args:
        url: è¦é©—è­‰çš„ URL

    Returns:
        URL æ˜¯å¦å®‰å…¨
    """
    try:
        parsed = urlparse(url)

        # åªå…è¨± http å’Œ https å”è­°
        if parsed.scheme not in ['http', 'https']:
            print(f"âš ï¸  ä¸æ”¯æ´çš„å”è­°ï¼š{parsed.scheme}")
            return False

        # ç¦æ­¢è¨ªå•å…§ç¶² IP å’Œ localhost
        hostname = parsed.hostname
        if not hostname:
            return False

        # ç¦æ­¢è¨ªå• localhost å’Œå…§ç¶² IP
        blocked_hosts = ['localhost', '127.0.0.1', '0.0.0.0']
        if hostname.lower() in blocked_hosts:
            print("âš ï¸  ç¦æ­¢è¨ªå• localhost")
            return False

        # ç¦æ­¢è¨ªå•å…§ç¶² IP æ®µï¼ˆ10.x.x.x, 172.16-31.x.x, 192.168.x.xï¼‰
        if hostname.startswith(('10.', '172.', '192.168.')):
            print("âš ï¸  ç¦æ­¢è¨ªå•å…§ç¶² IP")
            return False

        return True

    except Exception as e:
        print(f"âš ï¸  URL é©—è­‰å¤±æ•—ï¼š{e}")
        return False


def extract_104_job_content(soup: BeautifulSoup) -> str:
    """
    å°ˆé–€æå– 104 è·ç¼ºé é¢çš„å…§å®¹ã€‚

    Args:
        soup: BeautifulSoup ç‰©ä»¶

    Returns:
        æå–çš„è·ç¼ºè³‡è¨Š
    """
    info_parts = []

    try:
        # æå–è·ä½æ¨™é¡Œ
        title = soup.find('h1')
        if title:
            info_parts.append(f"è·ä½ï¼š{title.get_text(strip=True)}")

        # æå–å…¬å¸åç¨±
        company = soup.find('a', class_='company')
        if not company:
            company = soup.find(attrs={'data-qa': 'company-name'})
        if company:
            info_parts.append(f"å…¬å¸ï¼š{company.get_text(strip=True)}")

        # æå–è–ªè³‡
        salary = soup.find(attrs={'data-qa': 'salary'})
        if not salary:
            salary = soup.find('span', class_='salary')
        if salary:
            info_parts.append(f"è–ªè³‡ï¼š{salary.get_text(strip=True)}")

        # æå–å·¥ä½œåœ°é»
        location = soup.find(attrs={'data-qa': 'job-location'})
        if not location:
            location = soup.find('span', class_='location')
        if location:
            info_parts.append(f"åœ°é»ï¼š{location.get_text(strip=True)}")

        # æå–è·ä½æè¿°
        description = soup.find('div', class_='job-description')
        if not description:
            description = soup.find(attrs={'data-qa': 'job-description'})
        if description:
            desc_text = description.get_text(strip=True)[:500]  # é™åˆ¶é•·åº¦
            info_parts.append(f"è·ä½æè¿°ï¼š{desc_text}")

        return "\n".join(info_parts) if info_parts else ""

    except Exception as e:
        print(f"âš ï¸  104 å…§å®¹æå–å¤±æ•—ï¼š{e}")
        return ""


def extract_general_content(soup: BeautifulSoup) -> str:
    """
    æå–ä¸€èˆ¬ç¶²ç«™çš„ä¸»è¦å…§å®¹ã€‚

    Args:
        soup: BeautifulSoup ç‰©ä»¶

    Returns:
        æå–çš„æ–‡å­—å…§å®¹
    """
    try:
        # ç§»é™¤ script å’Œ style æ¨™ç±¤
        for script in soup(['script', 'style', 'header', 'footer', 'nav']):
            script.decompose()

        # æå–æ¨™é¡Œ
        title = soup.find('title')
        title_text = title.get_text(strip=True) if title else ""

        # æå– h1
        h1 = soup.find('h1')
        h1_text = h1.get_text(strip=True) if h1 else ""

        # æå–ä¸»è¦å…§å®¹å€åŸŸ
        main_content = soup.find('main') or soup.find('article') or soup.find('body')
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)

        # æ¸…ç†å¤šé¤˜ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()

        # çµ„åˆè³‡è¨Š
        info_parts = []
        if title_text:
            info_parts.append(f"é é¢æ¨™é¡Œï¼š{title_text}")
        if h1_text and h1_text != title_text:
            info_parts.append(f"ä¸»æ¨™é¡Œï¼š{h1_text}")
        if text:
            # é™åˆ¶å…§å®¹é•·åº¦
            content_preview = text[:1500]
            info_parts.append(f"å…§å®¹ï¼š{content_preview}")

        return "\n".join(info_parts)

    except Exception as e:
        print(f"âš ï¸  é€šç”¨å…§å®¹æå–å¤±æ•—ï¼š{e}")
        return ""


def is_dynamic_website(url: str) -> bool:
    """
    åˆ¤æ–·æ˜¯å¦ç‚ºéœ€è¦ JavaScript æ¸²æŸ“çš„å‹•æ…‹ç¶²ç«™ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™
    """
    # éœ€è¦ä½¿ç”¨ Playwright çš„ç¶²ç«™åˆ—è¡¨
    dynamic_domains = [
        '104.com.tw',
        # æœªä¾†å¯ä»¥æ·»åŠ æ›´å¤šéœ€è¦å‹•æ…‹æ¸²æŸ“çš„ç¶²ç«™
    ]
    return any(domain in url for domain in dynamic_domains)


async def extract_104_dynamic_content(page: Page) -> str:
    """
    ä½¿ç”¨ Playwright æå– 104 è·ç¼ºé é¢çš„å…§å®¹ã€‚

    Args:
        page: Playwright Page ç‰©ä»¶

    Returns:
        æå–çš„è·ç¼ºè³‡è¨Š
    """
    info_parts = []

    try:
        # ç­‰å¾…é é¢é—œéµå…ƒç´ è¼‰å…¥
        # 104 ç¶²ç«™é€šå¸¸æœƒæœ‰ job-header æˆ–é¡ä¼¼çš„å®¹å™¨
        await page.wait_for_selector('body', timeout=15000)

        # çµ¦äºˆé¡å¤–æ™‚é–“è®“ JavaScript æ¸²æŸ“å…§å®¹
        await page.wait_for_timeout(2000)

        # æå–è·ä½æ¨™é¡Œ - å˜—è©¦å¤šå€‹é¸æ“‡å™¨
        title_selectors = [
            'h1',
            '[data-qa="job-title"]',
            '.job-header__title',
            '.job__title'
        ]
        for selector in title_selectors:
            try:
                title = page.locator(selector).first
                if await title.count() > 0:
                    title_text = await title.inner_text()
                    if title_text:
                        info_parts.append(f"è·ä½ï¼š{title_text.strip()}")
                        break
            except:
                continue

        # æå–å…¬å¸åç¨±
        company_selectors = [
            '[data-qa="company-name"]',
            '.job-header__company',
            '.company-name'
        ]
        for selector in company_selectors:
            try:
                company = page.locator(selector).first
                if await company.count() > 0:
                    company_text = await company.inner_text()
                    if company_text:
                        info_parts.append(f"å…¬å¸ï¼š{company_text.strip()}")
                        break
            except:
                continue

        # æå–è–ªè³‡
        salary_selectors = [
            '[data-qa="salary"]',
            '.job-header__salary',
            '.salary'
        ]
        for selector in salary_selectors:
            try:
                salary = page.locator(selector).first
                if await salary.count() > 0:
                    salary_text = await salary.inner_text()
                    if salary_text:
                        info_parts.append(f"è–ªè³‡ï¼š{salary_text.strip()}")
                        break
            except:
                continue

        # æå–å·¥ä½œåœ°é»
        location_selectors = [
            '[data-qa="job-location"]',
            '.job-header__location',
            '.location'
        ]
        for selector in location_selectors:
            try:
                location = page.locator(selector).first
                if await location.count() > 0:
                    location_text = await location.inner_text()
                    if location_text:
                        info_parts.append(f"åœ°é»ï¼š{location_text.strip()}")
                        break
            except:
                continue

        # æå–è·ä½æè¿°
        desc_selectors = [
            '[data-qa="job-description"]',
            '.job-description',
            '.description'
        ]
        for selector in desc_selectors:
            try:
                description = page.locator(selector).first
                if await description.count() > 0:
                    desc_text = await description.inner_text()
                    if desc_text:
                        # é™åˆ¶é•·åº¦
                        desc_preview = desc_text.strip()[:500]
                        info_parts.append(f"è·ä½æè¿°ï¼š{desc_preview}")
                        break
            except:
                continue

        # å¦‚æœæ²’æœ‰æå–åˆ°ä»»ä½•å…§å®¹ï¼Œå˜—è©¦ç²å–æ•´å€‹é é¢çš„æ–‡å­—å…§å®¹
        if not info_parts:
            print("âš ï¸  æœªæ‰¾åˆ°ç‰¹å®šå…ƒç´ ï¼Œå˜—è©¦æå–æ•´é«”å…§å®¹")
            body_text = await page.locator('body').inner_text()
            # æ¸…ç†ä¸¦é™åˆ¶é•·åº¦
            body_text = re.sub(r'\s+', ' ', body_text).strip()[:1500]
            info_parts.append(f"é é¢å…§å®¹ï¼š{body_text}")

        return "\n".join(info_parts) if info_parts else ""

    except Exception as e:
        print(f"âš ï¸  104 å‹•æ…‹å…§å®¹æå–å¤±æ•—ï¼š{e}")
        return ""


async def fetch_dynamic_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ Playwright æå–å‹•æ…‹ç¶²ç«™å…§å®¹ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æå–çš„å…§å®¹ï¼Œå¤±æ•—å‰‡è¿”å› None
    """
    try:
        print(f"ğŸ­ ä½¿ç”¨ Playwright æå–å‹•æ…‹å…§å®¹ï¼š{url}")

        async with async_playwright() as p:
            # å•Ÿå‹•ç€è¦½å™¨ï¼ˆheadless æ¨¡å¼ï¼‰
            browser = await p.chromium.launch(headless=True)

            # å‰µå»ºä¸Šä¸‹æ–‡ï¼Œç¦ç”¨ä¸å¿…è¦çš„è³‡æºä»¥æé«˜é€Ÿåº¦
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                ignore_https_errors=False,  # é©—è­‰ SSL
            )

            # é˜»æ­¢ä¸å¿…è¦çš„è³‡æºè¼‰å…¥ä»¥æé«˜é€Ÿåº¦
            await context.route("**/*.{png,jpg,jpeg,gif,svg,css,font,woff,woff2}", lambda route: route.abort())

            # å‰µå»ºé é¢
            page = await context.new_page()

            # è¨­ç½®è¶…æ™‚
            page.set_default_timeout(30000)  # 30 ç§’

            # è¨ªå•é é¢ - ä½¿ç”¨ domcontentloaded è€Œä¸æ˜¯ networkidleï¼ˆæ›´å¿«ï¼‰
            await page.goto(url, wait_until='domcontentloaded', timeout=30000)

            # åˆ¤æ–·æ˜¯å¦ç‚º 104 ç¶²ç«™
            is_104 = '104.com.tw' in url

            if is_104:
                print("ğŸ“‹ ä½¿ç”¨ 104 å°ˆç”¨æå–å™¨")
                content = await extract_104_dynamic_content(page)
            else:
                print("ğŸ“„ ä½¿ç”¨é€šç”¨å‹•æ…‹å…§å®¹æå–å™¨")
                # é€šç”¨æå–ï¼šç­‰å¾…é é¢ç©©å®šå¾Œç²å–æ–‡å­—
                await page.wait_for_timeout(2000)
                body_text = await page.locator('body').inner_text()
                body_text = re.sub(r'\s+', ' ', body_text).strip()[:1500]
                content = f"é é¢å…§å®¹ï¼š{body_text}"

            # é—œé–‰ç€è¦½å™¨
            await browser.close()

            if not content:
                print("âš ï¸  æœªèƒ½æå–åˆ°æœ‰æ•ˆå…§å®¹")
                return None

            print(f"âœ… æˆåŠŸæå–å‹•æ…‹å…§å®¹ï¼ˆ{len(content)} å­—å…ƒï¼‰")
            return content

    except PlaywrightTimeout:
        print("âš ï¸  Playwright è«‹æ±‚è¶…æ™‚")
        return None
    except Exception as e:
        print(f"âš ï¸  Playwright æå–å¤±æ•—ï¼š{e}")
        return None


def fetch_static_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ requests + BeautifulSoup æå–éœæ…‹ç¶²ç«™å…§å®¹ï¼ˆåŸæœ‰é‚è¼¯ï¼‰ã€‚

    Args:
        url: ç¶²é  URL

    Returns:
        æå–çš„å…§å®¹ï¼Œå¤±æ•—å‰‡è¿”å› None
    """
    try:
        print(f"ğŸŒ ä½¿ç”¨éœæ…‹æ–¹æ³•æå–ç¶²é å…§å®¹ï¼š{url}")

        # è¨­å®šè«‹æ±‚æ¨™é ­
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        }

        # ç™¼é€è«‹æ±‚
        response = requests.get(
            url,
            headers=headers,
            timeout=10,
            allow_redirects=True,
            verify=True,
            stream=True
        )
        response.raise_for_status()

        # é™åˆ¶å…§å®¹å¤§å°
        content_length = response.headers.get('content-length')
        if content_length and int(content_length) > 5 * 1024 * 1024:
            print("âš ï¸  ç¶²é å…§å®¹éå¤§ï¼Œè¶…é 5MB é™åˆ¶")
            return None

        # ç²å–å…§å®¹
        content = b''
        size = 0
        max_size = 5 * 1024 * 1024
        for chunk in response.iter_content(chunk_size=8192):
            size += len(chunk)
            if size > max_size:
                print("âš ï¸  ä¸‹è¼‰å…§å®¹è¶…éå¤§å°é™åˆ¶")
                break
            content += chunk

        # è§£æ HTML
        soup = BeautifulSoup(content, 'html.parser')
        extracted_content = extract_general_content(soup)

        if not extracted_content:
            print("âš ï¸  æœªèƒ½æå–åˆ°æœ‰æ•ˆå…§å®¹")
            return None

        # é•·åº¦é™åˆ¶
        if len(extracted_content) > 2000:
            extracted_content = extracted_content[:2000] + "..."

        print(f"âœ… æˆåŠŸæå–éœæ…‹å…§å®¹ï¼ˆ{len(extracted_content)} å­—å…ƒï¼‰")
        return extracted_content

    except requests.Timeout:
        print("âš ï¸  è«‹æ±‚è¶…æ™‚")
        return None
    except requests.RequestException as e:
        print(f"âš ï¸  ç¶²é æå–å¤±æ•—ï¼š{e}")
        return None
    except Exception as e:
        print(f"âš ï¸  è™•ç†ç¶²é å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None


async def fetch_webpage_content(url: str) -> Optional[str]:
    """
    æ™ºèƒ½æå–ç¶²é å…§å®¹ï¼Œè‡ªå‹•é¸æ“‡éœæ…‹æˆ–å‹•æ…‹æå–æ–¹æ³•ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰ã€‚

    æµç¨‹ï¼š
    1. URL å®‰å…¨é©—è­‰
    2. åˆ¤æ–·æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™ï¼ˆå¦‚ 104.com.twï¼‰
    3. å‹•æ…‹ç¶²ç«™ â†’ ä½¿ç”¨ Playwrightï¼ˆç•°æ­¥ï¼‰
    4. éœæ…‹ç¶²ç«™ â†’ ä½¿ç”¨ requests + BeautifulSoup

    è³‡å®‰é˜²è­·æªæ–½ï¼š
    - URL å®‰å…¨é©—è­‰ï¼ˆé˜²æ­¢ SSRF æ”»æ“Šï¼‰
    - é™åˆ¶è«‹æ±‚å¤§å°
    - è¨­å®šè¶…æ™‚æ™‚é–“
    - é©—è­‰ SSL è­‰æ›¸

    Args:
        url: ç¶²é  URL

    Returns:
        ç¶²é çš„æ–‡å­—å…§å®¹æ‘˜è¦ï¼Œå¦‚æœå¤±æ•—å‰‡è¿”å› None
    """
    # è³‡å®‰æª¢æŸ¥ï¼šé©—è­‰ URL å®‰å…¨æ€§
    if not validate_url_security(url):
        print("âŒ URL å®‰å…¨é©—è­‰å¤±æ•—")
        return None

    # åˆ¤æ–·æ˜¯å¦ç‚ºå‹•æ…‹ç¶²ç«™
    if is_dynamic_website(url):
        # ä½¿ç”¨ Playwright æå–å‹•æ…‹å…§å®¹ï¼ˆç•°æ­¥ï¼‰
        return await fetch_dynamic_content(url)
    else:
        # ä½¿ç”¨éœæ…‹æ–¹æ³•æå–å…§å®¹
        return fetch_static_content(url)


async def analyze_job_detail(job_query: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LLM åˆ†æç‰¹å®šè·ç¼ºçš„è©³ç´°è³‡è¨Šã€‚

    æ­¤å‡½æ•¸åˆ©ç”¨ LLM å…§å»ºçš„ web search åŠŸèƒ½ï¼ŒæŸ¥è©¢è·ç¼ºç›¸é—œçš„å…¬å¸èƒŒæ™¯ã€
    è·ä½è¦æ±‚ã€å“¡å·¥è©•åƒ¹ç­‰è³‡è¨Šï¼Œä¸¦ç”Ÿæˆåˆ†æå ±å‘Šã€‚

    Args:
        job_query: è·ç¼ºæŸ¥è©¢è³‡è¨Šï¼Œæ‡‰åŒ…å«è·ä½åç¨±ã€å…¬å¸åç¨±ç­‰åŸºæœ¬æè¿°ã€‚
                  ç¯„ä¾‹ï¼šã€ŒæŸç§‘æŠ€å…¬å¸çš„ Python å¾Œç«¯å·¥ç¨‹å¸«ã€æˆ–
                        ã€Œ104 è·ç¼ºé€£çµï¼šhttps://www.104.com.tw/job/xxxxxã€

    Returns:
        åŒ…å«åˆ†æçµæœçš„å­—å…¸ï¼š
        {
            "job_query": str,                # è¼¸å…¥çš„è·ç¼ºæŸ¥è©¢
            "analysis_report": str,          # LLM ç”Ÿæˆçš„åˆ†æå ±å‘Š
            "processing_time": float,        # è™•ç†æ™‚é–“ï¼ˆç§’ï¼‰
            "token_usage": Dict,             # Token ä½¿ç”¨é‡çµ±è¨ˆ
        }

    Example:
        >>> result = analyze_job_detail("æŸç§‘æŠ€å…¬å¸çš„ Python å¾Œç«¯å·¥ç¨‹å¸«")
        >>> print(result["analysis_report"])

        >>> # ä¹Ÿå¯ä»¥ç›´æ¥å‚³å…¥ 104 é€£çµ
        >>> result = analyze_job_detail("https://www.104.com.tw/job/xxxxx")
        >>> print(result["analysis_report"])
    """

    # æº–å‚™è¿”å›çµæœ
    result = {
        "job_query": job_query,
        "analysis_report": "",
        "processing_time": 0.0,
        "token_usage": {},
        "webpage_content": None,  # æ–°å¢ï¼šå„²å­˜æå–çš„ç¶²é å…§å®¹
    }

    # æª¢æŸ¥æ˜¯å¦åŒ…å« URL
    url = extract_url_from_query(job_query)
    webpage_content = None

    if url:
        # å¦‚æœåŒ…å« URLï¼Œå…ˆæå–ç¶²é å…§å®¹
        webpage_content = await fetch_webpage_content(url)
        result["webpage_content"] = webpage_content

    # æº–å‚™è·ç¼ºè³‡è¨Š
    if webpage_content:
        # å¦‚æœæˆåŠŸæå–ç¶²é å…§å®¹ï¼Œå°‡å…¶åŠ å…¥æŸ¥è©¢è³‡è¨Š
        formatted_info = f"""ç”¨æˆ¶æŸ¥è©¢ï¼š{job_query}

å¾è·ç¼ºç¶²é æå–çš„åŸºæœ¬è³‡è¨Šï¼š
{webpage_content}

è«‹åŸºæ–¼ä»¥ä¸Šè³‡è¨Šé€²è¡Œæ›´æ·±å…¥çš„åˆ†æã€‚"""
    else:
        # æ²’æœ‰ç¶²é å…§å®¹ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è©¢
        formatted_info = job_query

    # ä½¿ç”¨ LLM é€²è¡Œåˆ†æ
    print("ğŸ¤– åˆ†æè·ç¼ºè©³ç´°è³‡è¨Šä¸­...")

    messages = [
        {
            "role": "system",
            "content": JOB_DETAIL_ANALYSIS_PROMPT.format(job_info=formatted_info),
        },
        {
            "role": "user",
            "content": "è«‹é–‹å§‹åˆ†æé€™å€‹è·ç¼ºã€‚",
        },
    ]

    start_time = time.time()
    llm_response = call_llm(messages=messages)
    processing_time = time.time() - start_time

    result["processing_time"] = processing_time

    # é¡¯ç¤ºè™•ç†æ™‚é–“å’Œ token ä½¿ç”¨é‡
    print(f"â±ï¸  åˆ†æè€—æ™‚: {processing_time:.2f} ç§’")
    if "usage" in llm_response:
        usage = llm_response["usage"]
        result["token_usage"] = {
            "total": usage.get("total_tokens", 0),
            "prompt": usage.get("prompt_tokens", 0),
            "completion": usage.get("completion_tokens", 0),
        }
        print(f"ğŸ“Š Token ä½¿ç”¨é‡: {usage.get('total_tokens', 0)} tokens "
              f"(prompt: {usage.get('prompt_tokens', 0)}, "
              f"completion: {usage.get('completion_tokens', 0)})")

    # æå–åˆ†æå ±å‘Š
    result["analysis_report"] = llm_response["choices"][0]["message"]["content"]
    print(f"ğŸ“ å ±å‘Šé•·åº¦ï¼š{len(result['analysis_report'])} å­—å…ƒ")

    return result
